{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 185 Final Project: Auto Context (2, 2.5k, 2.5k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: dlib in /home/rrdu/.local/lib/python3.9/site-packages (19.24.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "oe = preprocessing.OneHotEncoder(sparse=False)\n",
    "import dlib\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "import timeit\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows Length\n",
    "L = 2\n",
    "# Number of examples\n",
    "N = 5000\n",
    "# Length of a feature\n",
    "d = 128\n",
    "# The hyper-parameter for icm search\n",
    "Niter = 2     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2i(a):\n",
    "        return int(ord(a)-ord('a'))\n",
    "def i2l(i):\n",
    "    if i >= 0:\n",
    "        return chr(i+ord('a'))\n",
    "    else:\n",
    "        return '_'\n",
    "def iors(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError: # if it is a string, return a string\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the entire dataset into lists or list of lists\n",
    "def read_OCR(filename, n_features):\n",
    "    F = open(filename)\n",
    "    dataset = {}\n",
    "    dataset['ids'] = []#np.zeros(n_examples, dtype=int)\n",
    "    dataset['labels'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['labelDic'] = {} # To profile the distribution of labels\n",
    "    dataset['next_ids'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['word_ids'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['positions'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['folds'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['features'] = []#np.zeros([n_examples,n_features])\n",
    "    \n",
    "    for str_line in F.readlines():\n",
    "        #line0 = map(iors, filter(None, re.split('\\t', str_line.strip())))\n",
    "        ## ATTENTION: If you are using Python3, use the following line instead\n",
    "        line0 = list(map(iors, filter(None, re.split('\\t', str_line.strip()))))\n",
    "\n",
    "\n",
    "        dataset['ids'].append(int(line0.pop(0)))\n",
    "        dataset['labels'].append(l2i(line0.pop(0))) # The label is converted into integer('a'=>0, 'z'=>25)\n",
    "        if dataset['labels'][-1] in dataset['labelDic']:\n",
    "            dataset['labelDic'][dataset['labels'][-1]] += 1\n",
    "        else:\n",
    "            dataset['labelDic'][dataset['labels'][-1]] = 1\n",
    "            \n",
    "        dataset['next_ids'].append(int(line0.pop(0)))\n",
    "        dataset['word_ids'].append(int(line0.pop(0)))\n",
    "        dataset['positions'].append(int(line0.pop(0)))\n",
    "        dataset['folds'].append(int(line0.pop(0)))\n",
    "        if len(line0) != 128:  # Sanity check of the length\n",
    "            print (len(line0))\n",
    "        dataset['features'].append(line0)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = read_OCR('OCRdataset/letter.data', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Concatenating and Structurizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Find 5000 words, split them into 2500 for training, and 2500 for testing.\n",
    "\n",
    "2) Chop first 2 characters from each word.\n",
    "\n",
    "3) Now there are 2500 two-words pairs for training, and 2500 for testing.\n",
    "\n",
    "4) Construct new structures data based on this pairs.\n",
    "- Ex: \n",
    "    - apple => ap\n",
    "    - banana => ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first L letters in a word\n",
    "\n",
    "def structurize1(dataset, N, L):\n",
    "    d_features = len(dataset['features'][0])\n",
    "    y = dataset['labels']\n",
    "    X = dataset['features']\n",
    "    next_id = dataset['next_ids']\n",
    "\n",
    "    labels = np.zeros((N, L))\n",
    "    features = np.zeros((N, L * d_features))\n",
    "    \n",
    "    def extract(iN, loc):\n",
    "        labels[iN] = y[loc:loc + L]\n",
    "        features[iN] = np.array(X[loc:loc + L]).ravel().tolist()\n",
    "        iN += 1\n",
    "        return iN\n",
    "    \n",
    "    iN = 0\n",
    "    iN = extract(iN, 0)\n",
    "    \n",
    "    for key, value in enumerate(y):\n",
    "        if next_id[key] == -1:\n",
    "            iN = extract(iN, key + 1)\n",
    "            if iN == N:\n",
    "                break\n",
    "    \n",
    "    c = list(zip(labels, features))\n",
    "    random.shuffle(c)\n",
    "    labels, features = zip(*c)\n",
    "    \n",
    "    return np.array(labels), np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1, features1 = structurize1(dataset1, N, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: dlib Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoContextProblem:\n",
    "    C = 1\n",
    "\n",
    "    def __init__(self, samples, labels, L, K, d, Niter=2):\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        self.d = d\n",
    "        self.num_samples = len(samples)\n",
    "        self.num_dimensions = (L * K * d + 1) + (L - 1)\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.context = np.zeros((len(samples), L * K), dtype=float)  # Initialize context as float\n",
    "        self.loss_for_loop = True\n",
    "        self.Niter = Niter  # Define Niter attribute\n",
    "\n",
    "    def make_psi(self, x, label, context):\n",
    "        psi = dlib.vector()\n",
    "        psi.resize(self.num_dimensions)\n",
    "        psi[0] = float(x[0])  # The bias\n",
    "\n",
    "        for i in range(self.L):\n",
    "            x_offset = 128 * i\n",
    "            x_offset += 1\n",
    "            psi_offset = label[i] * 128\n",
    "            psi_offset += i * self.K * self.d\n",
    "            psi_offset += 1\n",
    "            for j in range(self.d):\n",
    "                psi[psi_offset + j] = float(x[x_offset + j])\n",
    "\n",
    "            # Incorporate context\n",
    "            context_index = -(self.L * self.K) + i * self.K + label[i]\n",
    "            context_value = float(context[i * self.K + label[i]])  # Ensure context value is a float\n",
    "            psi[context_index] = context_value\n",
    "\n",
    "        if label[0] == label[1]:\n",
    "            psi[-(self.L * self.K) - 1] = 0.0\n",
    "        else:\n",
    "            psi[-(self.L * self.K) - 1] = 1.0\n",
    "        return psi\n",
    "\n",
    "    def get_truth_joint_feature_vector(self, idx):\n",
    "        return self.make_psi(self.samples[idx], self.labels[idx], self.context[idx])\n",
    "\n",
    "    def separation_oracle(self, idx, current_solution):\n",
    "        samp = self.samples[idx]\n",
    "        psi = [0] * self.num_dimensions\n",
    "        max1 = -1e10\n",
    "        max_scoring_label = [0] * self.L\n",
    "        for k in range(self.Niter):  # Use self.Niter\n",
    "            for iL in range(self.L):\n",
    "                for i in range(self.K):\n",
    "                    tmp_label = max_scoring_label.copy()\n",
    "                    tmp_label[iL] = i\n",
    "                    tmp_psi = self.make_psi(samp, tmp_label, self.context[idx])\n",
    "                    score1 = dlib.dot(current_solution, tmp_psi)\n",
    "\n",
    "                    loss1 = 0.0\n",
    "                    if self.loss_for_loop:\n",
    "                        for j in range(self.L):\n",
    "                            if self.labels[idx][j] != tmp_label[j]:\n",
    "                                loss1 += 1.0\n",
    "                    else:\n",
    "                        if self.labels[idx] != tmp_label:\n",
    "                            loss1 += 1.0\n",
    "\n",
    "                    if max1 < score1 + loss1:\n",
    "                        max1 = score1 + loss1\n",
    "                        loss = loss1\n",
    "                        max_scoring_label[iL] = i\n",
    "                        psi = tmp_psi\n",
    "\n",
    "        return loss, psi\n",
    "\n",
    "    def update_context(self, samples, labels, weights):\n",
    "        for idx, samp in enumerate(samples):\n",
    "            prediction = [0] * self.L\n",
    "            for iL in range(self.L):\n",
    "                max_score = -1e10\n",
    "                for i in range(self.K):\n",
    "                    tmp_label = prediction.copy()\n",
    "                    tmp_label[iL] = i\n",
    "                    psi1 = self.make_psi(samp, tmp_label, self.context[idx])\n",
    "                    score1 = dlib.dot(weights, psi1)\n",
    "\n",
    "                    if max_score < score1:\n",
    "                        max_score = score1\n",
    "                        prediction[iL] = i\n",
    "\n",
    "            self.context[idx] = np.array([1.0 if l == labels[idx][i] else 0.0 for i, l in enumerate(prediction) for _ in range(self.K)], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(samples, labels, problem, weights, K):\n",
    "    predictions = []\n",
    "    for idx, samp in enumerate(samples):\n",
    "        prediction = [0] * problem.L\n",
    "        for iL in range(problem.L):\n",
    "            max_score = -1e10\n",
    "            for i in range(K):\n",
    "                tmp_label = prediction.copy()\n",
    "                tmp_label[iL] = i\n",
    "                if idx < len(problem.context):  # Ensure idx is within bounds\n",
    "                    psi1 = problem.make_psi(samp, tmp_label, problem.context[idx])\n",
    "                    score1 = dlib.dot(weights, psi1)\n",
    "\n",
    "                    if max_score < score1:\n",
    "                        max_score = score1\n",
    "                        prediction[iL] = i\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    errCnt = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != labels[i]:\n",
    "            errCnt += 1\n",
    "\n",
    "    return 1.0 - float(errCnt) / float(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le1 = preprocessing.LabelEncoder()\n",
    "nplabels1 = le1.fit_transform(labels1.ravel()).reshape(labels1.shape)\n",
    "npsamples1 = np.hstack([np.ones((N, 1)), features1])\n",
    "K1 = len(le1.classes_)\n",
    "\n",
    "tr_labels = nplabels1[int(N*0.5):].astype(int).tolist()\n",
    "tr_samples = npsamples1[int(N*0.5):].astype(int).tolist()\n",
    "te_labels = nplabels1[:int(N*0.5)].astype(int).tolist()\n",
    "te_samples = npsamples1[:int(N*0.5)].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Auto-Context Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time elapsed: 322.1306966559496 s\n"
     ]
    }
   ],
   "source": [
    "problem = AutoContextProblem(tr_samples, tr_labels, L, K1, d, Niter=2)\n",
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print(\"Training time elapsed:\", end_train - start_train, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.49160000000000004\n",
      "Test accuracy= 0.4556\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(weights, open('auto_context_weights2_2.5_2.5.obj', 'wb'))\n",
    "weights_load = pickle.load(open('auto_context_weights2_2.5_2.5.obj', 'rb'))\n",
    "\n",
    "print(\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K1))\n",
    "print(\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
